mean(og_data[mask])
require(splatter)
require(scater)
### Parameters (choose wisely) ###
random_seed <- 43
num_cells <- 1000
num_genes <- 5000
num_groups <- 5
dropout <- 1 #boolean valued for now
dropout.mid.value <- 2
dropout.shape.value <- -1
PATHNAME <- paste("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/",
num_groups,"_groups_", num_cells, "_cells_", num_genes, "_genes/", sep="")
### Run simulation ###
params <- newSplatParams()
params <- setParam(params, "seed", random_seed)
params <- setParam(params, "batchCells", num_cells)
params <- setParam(params, "nGenes", num_genes)
if(num_groups > 1) {
# Uniformly distributed groups
probabilities = rep(1/num_groups, num_groups)
params <- setParam(params, "group.prob", probabilities)
if (dropout) {
dropout.mid = rep(dropout.mid.value, num_groups) #default, possibly a bad assumption
dropout.shape = rep(dropout.shape.value, num_groups) #default, possibly a bad assumption
params <- setParam(params, "dropout.mid", dropout.mid)
params <- setParam(params, "dropout.shape", dropout.shape)
params <- setParam(params, "dropout.type", "group")
}
sim <- splatSimulate(params, method = "groups")
} else {
if (dropout) {
params <- setParam(params, "dropout.mid", dropout.mid.value)
params <- setParam(params, "dropout.shape", dropout.shape.value)
params <-setParam(params, "dropout.type", "experiment")
}
sim <- splatSimulate(params)
}
sum(rowSums(counts(sim) == 0))
sim <- logNormCounts(sim)
View(sim)
install.packages("hrbrthemes")
library(softImpute)
source("../ALRA/alra.r")
source('../ALRA/alra.r')
source('../ALRA/alra.R')
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
normalize_rows_by_noise = 0 #boolean
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_data <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_data
true_data <- t(normalize_data(t(true_data)))
data[mask] = 0
data <- t(normalize_data(t(data)))
data[mask] = NA
histogram((true_data - data)[!mask])
hist((true_data - data)[!mask])
library(ggplot2)
data1 <- (true_data - data)[!mask]
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
output <- complete(data, fits)
data2 <- (true_data - output)[mask]
p1 <- data1
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
p1 <- as.data.frame(data1)
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
par(mfcol=c(2,1))
hist(true_data - data)[!mask]
hist((true_data-output)[mask])
par(mfcol=c(2,1))
hist((true_data - data)[!mask])
hist((true_data - data)[mask])
hist((true_data - output)[mask])
mean(abs(output[mask]))
sd(abs(output[mask]))
mean(output[mask])
mean((true_data-output)[mask])
sd((true_data-output)[mask])
mean(true_data[mask])
sd(true_data[mask])
# Set working directory, load libraries, and get source.
setwd('~/Documents/Projects/ZoomDeflate/')
library(ggplot2)
library(dplyr)
library(hrbrthemes)
source('ALRA/alra.R')
set.seed(41) # Some ML projects make this seed a hyper-parameter
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Jeremy's computer, fix soon ###
PATHNAME = "~/Documents/Projects/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME,"dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME,"counts.csv", sep="")
TRUE_PATHNAME = paste(PATHNAME,"true_counts.csv",sep="")
# load matrices in format: rows are genes, columns are cells
mask <- as.matrix(read.csv(MASK_PATHNAME, header=FALSE, sep=" "))
data <- as.matrix(read.csv(DATA_PATHNAME, header=FALSE, sep=" "))
truth <- as.matrix(read.csv(TRUE_PATHNAME, header=FALSE, sep=" "))
# take transpose b/c that's what ALRA likes
mask <- t(mask)
data <- t(data)
truth <- t(truth)
truth <- normalize_data(truth)
# turn mask into logical array for logical indexing
mask <- mask == 1
# Library and log normalize the data
A_norm <- normalize_data(data)
# Choose k (# of singular values in the approximation) by measuring when they get smol.
k_choice <- choose_k(A_norm)
# print(k_choice)
# complete matrix using ALRA
# A_norm_completed <- alra(A_norm,k=k_choice$k)[[3]]
A_norm_completed <- alra(A_norm,k=5)[[3]]
# Calculate RMSE for all values,
difference <- A_norm_completed - truth
RMSE_all <- sqrt(sum(difference^2) / (length(truth)))
print(RMSE_all)
reconned_dropouts <- A_norm_completed[mask]
truth_dropouts <- truth[mask]
RMSE_reconned <- sqrt(sum((reconned_dropouts - truth_dropouts)^2) / length(truth_dropouts))
print(RMSE_reconned)
reconned_nondropped <- A_norm_completed[!mask]
truth_nondropped <- truth[!mask]
RMSE_nondropped <- sqrt(sum((reconned_nondropped - truth_nondropped)^2) / length(truth_nondropped))
print(RMSE_nondropped)
# histogram of dropout entries
data <- data.frame(
type = c( rep("Reconned dropouts", length(reconned_dropouts)), rep("True dropouts", length(truth_dropouts))),
value = c( log(reconned_dropouts), log(truth_dropouts) )
)
p1 <- data %>%
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
# histogram of nondropout entries
data2 <- data.frame(
type = c( rep("Reconned nondropped", length(reconned_nondropped)), rep("True nondropped", length(truth_nondropped))),
value = c( reconned_nondropped, truth_nondropped )
)
p2 <- data2 %>%
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
# histogram of all entries
data3 <- data.frame(
type = c( rep("Truth", length(truth)), rep("Reconned", length(A_norm_completed) )),
value = c( truth, A_norm_completed) )
p3 <- data3 %>%
ggplot( aes(x=value, fill=type)) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins=100) +
scale_fill_manual(values=c("#69b3a2", "#404080")) +
theme_ipsum() +
labs(fill="")
print("I'm done running")
setwd('~/Documents/College/Math \651/ZoomDeflate/')
library(ggplot2)
library(dplyr)
library(hrbrthemes)
source('ALRA/alra.R')
library(softImpute)
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
normalize_rows_by_noise = 0 #boolean
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
true_counts <- t(normalize_data(t(true_counts)))
data[mask] = 0
data <- t(normalize_counts(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
output <- complete(data, fits)
diff_squared = (output - og_data)^2
print("RMSE on unknown entries: ")
sqrt(mean(diff_squared[mask]))
library(softImpute)
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
data[mask] = 0
# Collect column sums to invert the normalize_data operation
col_sums_data = colSums(data)
if (any(col_sums_data == 0)) {
toRemove <- which(col_sums_data == 0)
data <- data[,-toRemove]
col_sums_data <- col_sums_data[-toRemove]
}
data <- t(normalize_data(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
output <- complete(data, fits)
output <-  (exp(output) - 1)/10E3
# diff_squared = (output - og_data)^2 #This isn't right.
diff_squared = (output - true_counts)^2
hist(diff_squared)
hist(diff_squared, nbins = 10)
hist(diff_squared, nclass = 10)
head(diff_squared)[1:5, 1:5]
head(true_counts)[1:5, 1:5]
output <- complete(data, fits)
head(output)[1:5, 1:5]
output <-  (exp(output) - 1)/10E3
head(output)[1:5, 1:5]
head(data)[1:5,1:5]
output <- complete(data, fits)
output <-  (exp(output) - 1)/10E3
output <- sweep(output, 2, col_sums_data, '*')
head(output)[1:5, 1:5]
head(true_data)[1:5, 1:5]
head(data)[1:5,1:5]
library(softImpute)
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
data[mask] = 0
# Collect column sums to invert the normalize_data operation
col_sums_data = colSums(data)
if (any(col_sums_data == 0)) {
toRemove <- which(col_sums_data == 0)
data <- data[,-toRemove]
col_sums_data <- col_sums_data[-toRemove]
}
data <- t(normalize_data(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
output <- complete(data, fits)
output <-  (exp(output) - 1)/10E3
output <- sweep(output, 2, col_sums_data, '*')
# diff_squared = (output - og_data)^2 #This isn't right.
head(true_counts)[1:5, 1:5]
head(output)[1:5,1:5]
head(dropout)[1:5, 1:5]
head(mask)[1:5, 1:5]
head(data)[1:5, 1:5]
library(softImpute)
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
unnormalize_data <- function(A, og_col_sums) {
# invert the function normalize_data from alra.R
A <- (exp(A) - 1)/1E4
A <- sweep(A, 2, col_sums_data)
return(A)
}
myColSums <- function(A) {
# get column sums, meant to work specifically with normalize_data from alra.R
col_sums= colSums(A)
if (any(col_sums == 0)) {
toRemove <- which(col_sums == 0)
data <- data[,-toRemove]
col_sums <- col_sums[-toRemove]
}
return(col_sums)
}
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
data[mask] = 0
# Collect column sums to invert the normalize_data operation
col_sums_data = myColSums(data)
data <- t(normalize_data(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
# compute normalized output, then rescale so it is comparable with true_counts
output <- complete(data, fits)
output <-  unnormalize_data(output, col_sums_data)
diff = (output - true_data)[mask]
diff_squred = diff^2
sqrt(mean(diff_squared))
hist(diff_squared)
max(diff_squared)
sqrt(diff_squred)
diff <- sqrt(diff_squared)
hist(diff)
max(diff)
min(output)
hist(output)
hist(col_sums_data)
library(softImpute)
source("/Users/nikhil/Documents/College/Math 651/ZoomDeflate/ALRA/alra.R")
unnormalize_data <- function(A, og_col_sums) {
# invert the function normalize_data from alra.R
A <- (exp(A) - 1)/1E4
A <- sweep(A, 2, col_sums_data, '*')
return(A)
}
myColSums <- function(A) {
# get column sums, meant to work specifically with normalize_data from alra.R
col_sums= colSums(A)
if (any(col_sums == 0)) {
toRemove <- which(col_sums == 0)
data <- data[,-toRemove]
col_sums <- col_sums[-toRemove]
}
return(col_sums)
}
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
data[mask] = 0
# Collect column sums to invert the normalize_data operation
col_sums_data = myColSums(data)
data <- t(normalize_data(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
# compute normalized output, then rescale so it is comparable with true_counts
output <- complete(data, fits)
output <-  unnormalize_data(output, col_sums_data)
hist(output)
min(output)
max(output)
diff_squared = (true_values - output)
diff_squared = (true_data - output)
diff_squared = (true_data - output)^2
max(diff_squared)
diff <- sqrt(diff_squared)
hist(diff)
max(diff)
quantile(diff)
quantile(diff, probs = seq(0,1,0.1))
sd(true_counts)
mean(true_counts)
hist(diff[mask])
quantile(diff[mask])
quantile(diff[mask], probs = seq(0,1,0.1))
# Nikhil wd
setwd('~/Documents/College/Math \651/ZoomDeflate/')
library(softImpute)
source("../ALRA/alra.R")
source("jp_utilities.R")
# Currently cheating by using the true dropouts as a mask
# In the future, use predicted dropouts as a mask
### PATHNAME is currently for Nikhil's computer, fix soon ###
nCells = 1000
nGenes = 5000
PATHNAME = "/Users/nikhil/Documents/College/Math 651/ZoomDeflate/SplatGenData/5_groups_1000_cells_5000_genes/"
MASK_PATHNAME = paste(PATHNAME, "dropouts.csv", sep="")
DATA_PATHNAME = paste(PATHNAME, "true_counts.csv", sep="")
set.seed(1011)# Some ML projects make this seed a hyper-parameter
lambda_= 1.9  # regularization weight emphasizing the nuclear norm
type_ = "als" # either als or svd, als is faster
rank.max_ = 0 # not a necessary argument,
# but an estimate could be useful
mask <- as.matrix(read.table(MASK_PATHNAME), nrow = nCells, ncol = nGenes)
mask <- as.logical(mask)
true_counts <- as.matrix(read.table(DATA_PATHNAME), nrow = nCells, ncol = nGenes)
#normalize in the correct order
data <- true_counts
data[mask] = 0
# Collect column sums to invert the normalize_data operation
col_sums_data = myColSums(data)
data <- t(normalize_data(t(data)))
data[mask] = NA
if (rank.max_) {
fits = softImpute(data, rank.max = rank.max_,
lambda = lambda_, trace=TRUE, type=type_)
} else {
fits = softImpute(data, lambda = lambda_,
trace=TRUE, type=type_)
}
# compute normalized output, then rescale so it is comparable with true_counts
output <- complete(data, fits)
output <-  unnormalize_data(output, col_sums_data)
setwd('/Users/nikhil/Documents/College/Math 651/ZoomDeflate/softImputeRecon')
library(softImpute)
source("../ALRA/alra.R")
source("jp_utilities.R")
library(softImpute)
source("../ALRA/alra.R")
source("../ALRA/jp_utilities.R")
